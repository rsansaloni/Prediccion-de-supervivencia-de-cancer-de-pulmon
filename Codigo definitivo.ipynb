{"cells":[{"cell_type":"markdown","source":["# 1. Carpetas base e instalación de librerías"],"metadata":{"id":"VO-1HrWes5Fz"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"i57Sj5KZpJ4T","executionInfo":{"status":"ok","timestamp":1699894731021,"user_tz":-60,"elapsed":7,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"outputs":[],"source":["DATASET_FOLDER = '/content/drive/MyDrive/Lung_Dataset'\n","IMAGES_FOLDER = '/content/drive/MyDrive/Lung_Dataset/Imagenes'\n","IMAGES_NRRD_FOLDER = '/content/drive/MyDrive/Lung_Dataset/Imagenes_NRRD'\n","IMAGES_PRED_NRRD_FOLDER = '/content/drive/MyDrive/Lung_Dataset/Imagenes_Pred_NRRD'\n","SEGMENTATION_CODE_FOLDER = '/content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion'\n","MODEL_FOLDER = '/content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Model'"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install scikeras\n","!pip install SimpleITK\n","!pip install -r '/content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt'\n","!pip install pydicom\n","!pip install pyplastimatch\n","!pip install dcm2niix\n","!pip install pydicom_seg\n","!pip install dicom2nifti\n","!pip install pyradiomics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93AyNnNEgm3J","executionInfo":{"status":"ok","timestamp":1699894870922,"user_tz":-60,"elapsed":139906,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}},"outputId":"e14b40a8-d40f-407b-8080-75d6b1bcc123"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting scikeras\n","  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n","Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.2)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n","Installing collected packages: scikeras\n","Successfully installed scikeras-0.12.0\n","Collecting SimpleITK\n","  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: SimpleITK\n","Successfully installed SimpleITK-2.3.1\n","Requirement already satisfied: notebook>=6.1.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (6.5.5)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 2)) (1.23.5)\n","Requirement already satisfied: pandas>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 3)) (1.5.3)\n","Requirement already satisfied: opencv-python>=4.5.1.48 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 4)) (4.8.0.76)\n","Requirement already satisfied: keras>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 5)) (2.14.0)\n","Requirement already satisfied: pillow>=8.0.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 6)) (9.4.0)\n","Collecting pydicom>=1.3.0 (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 7))\n","  Downloading pydicom-2.4.3-py3-none-any.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pywavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 8)) (1.4.1)\n","Requirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (7.7.1)\n","Requirement already satisfied: scikit-image>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 10)) (0.19.3)\n","Requirement already satisfied: scikit-learn>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 11)) (1.2.2)\n","Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 12)) (1.11.3)\n","Requirement already satisfied: seaborn>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 13)) (0.12.2)\n","Requirement already satisfied: setuptools>=50.3.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 14)) (67.7.2)\n","Requirement already satisfied: simpleitk>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 15)) (2.3.1)\n","Collecting statannot>=0.2.3 (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 16))\n","  Downloading statannot-0.2.3-py3-none-any.whl (10 kB)\n","Requirement already satisfied: statsmodels>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 17)) (0.14.0)\n","Requirement already satisfied: tensorflow>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (2.14.0)\n","Requirement already satisfied: tqdm>=4.50.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 19)) (4.66.1)\n","Collecting xlrd==1.2.0 (from -r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 20))\n","  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (3.1.2)\n","Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (6.3.2)\n","Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (23.2.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (23.1.0)\n","Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (5.7.1)\n","Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (5.5.0)\n","Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (6.1.12)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.2.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (5.9.2)\n","Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (6.5.4)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (1.5.8)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (5.5.6)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (1.8.2)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.17.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.18.0)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (1.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 3)) (2023.3.post1)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (3.6.6)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (7.34.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (3.0.9)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17.2->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 10)) (3.2.1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17.2->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 10)) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17.2->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 10)) (2023.9.26)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17.2->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 10)) (23.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.2->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 11)) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.2->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 11)) (3.2.0)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn>=0.11.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 13)) (3.7.1)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.0->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 17)) (0.5.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (3.20.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (1.59.2)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (0.41.3)\n","Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9))\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (3.0.39)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (4.8.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (3.11.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 13)) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 13)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 13)) (4.44.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 13)) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 13)) (3.1.1)\n","Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (1.24.0)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.2.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (4.9.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (4.11.2)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (6.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (2.1.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.9.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (1.5.0)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (1.2.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (2.18.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (4.19.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (3.0.1)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (21.2.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (1.3.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (0.8.3)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.12.0)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (1.6.4)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 9)) (0.2.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (2023.7.22)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (2.5)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (0.5.1)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (1.1.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=6.1.4->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 1)) (2.21)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow>=2.4.1->-r /content/drive/MyDrive/Codigo_TFM/Lung/Codigo_segunda_entrega/Segmentacion/requirements.txt (line 18)) (3.2.2)\n","Installing collected packages: xlrd, statannot, pydicom, jedi\n","  Attempting uninstall: xlrd\n","    Found existing installation: xlrd 2.0.1\n","    Uninstalling xlrd-2.0.1:\n","      Successfully uninstalled xlrd-2.0.1\n","Successfully installed jedi-0.19.1 pydicom-2.4.3 statannot-0.2.3 xlrd-1.2.0\n","Requirement already satisfied: pydicom in /usr/local/lib/python3.10/dist-packages (2.4.3)\n","Collecting pyplastimatch\n","  Downloading pyplastimatch-0.4.3-py3-none-any.whl (13 kB)\n","Collecting itk (from pyplastimatch)\n","  Downloading itk-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (8.3 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pyplastimatch) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyplastimatch) (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pyplastimatch) (1.5.3)\n","Requirement already satisfied: pydicom in /usr/local/lib/python3.10/dist-packages (from pyplastimatch) (2.4.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pyplastimatch) (2.31.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from pyplastimatch) (0.19.3)\n","Requirement already satisfied: SimpleITK in /usr/local/lib/python3.10/dist-packages (from pyplastimatch) (2.3.1)\n","Collecting itk-core==5.3.0 (from itk->pyplastimatch)\n","  Downloading itk_core-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (81.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itk-numerics==5.3.0 (from itk->pyplastimatch)\n","  Downloading itk_numerics-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (58.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itk-io==5.3.0 (from itk->pyplastimatch)\n","  Downloading itk_io-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (25.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.6/25.6 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itk-filtering==5.3.0 (from itk->pyplastimatch)\n","  Downloading itk_filtering-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (73.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itk-registration==5.3.0 (from itk->pyplastimatch)\n","  Downloading itk_registration-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (26.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itk-segmentation==5.3.0 (from itk->pyplastimatch)\n","  Downloading itk_segmentation-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (16.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyplastimatch) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyplastimatch) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyplastimatch) (4.44.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyplastimatch) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyplastimatch) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyplastimatch) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyplastimatch) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyplastimatch) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pyplastimatch) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pyplastimatch) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pyplastimatch) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pyplastimatch) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pyplastimatch) (2023.7.22)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->pyplastimatch) (1.11.3)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->pyplastimatch) (3.2.1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->pyplastimatch) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->pyplastimatch) (2023.9.26)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->pyplastimatch) (1.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->pyplastimatch) (1.16.0)\n","Installing collected packages: itk-core, itk-numerics, itk-io, itk-filtering, itk-segmentation, itk-registration, itk, pyplastimatch\n","Successfully installed itk-5.3.0 itk-core-5.3.0 itk-filtering-5.3.0 itk-io-5.3.0 itk-numerics-5.3.0 itk-registration-5.3.0 itk-segmentation-5.3.0 pyplastimatch-0.4.3\n","Collecting dcm2niix\n","  Downloading dcm2niix-1.0.20220715.tar.gz (451 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.4/451.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting miutil[web] (from dcm2niix)\n","  Downloading miutil-0.12.0-py3-none-any.whl (18 kB)\n","Requirement already satisfied: tqdm>=4.40.0 in /usr/local/lib/python3.10/dist-packages (from miutil[web]->dcm2niix) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from miutil[web]->dcm2niix) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->miutil[web]->dcm2niix) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->miutil[web]->dcm2niix) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->miutil[web]->dcm2niix) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->miutil[web]->dcm2niix) (2023.7.22)\n","Building wheels for collected packages: dcm2niix\n","  Building wheel for dcm2niix (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dcm2niix: filename=dcm2niix-1.0.20220715-cp310-cp310-linux_x86_64.whl size=559552 sha256=9e0ef4f66bfda9e9cdc08000ecb638df1a8fdc83a1c1a8995534f9075e0d1586\n","  Stored in directory: /root/.cache/pip/wheels/88/8d/9b/5ea20c0451a1acddef585757be7dfec121ee076e58503b267c\n","Successfully built dcm2niix\n","Installing collected packages: miutil, dcm2niix\n","Successfully installed dcm2niix-1.0.20220715 miutil-0.12.0\n","Collecting pydicom_seg\n","  Downloading pydicom_seg-0.4.1-py3-none-any.whl (27 kB)\n","Requirement already satisfied: SimpleITK>1.2.4 in /usr/local/lib/python3.10/dist-packages (from pydicom_seg) (2.3.1)\n","Collecting jsonschema<4.0.0,>=3.2.0 (from pydicom_seg)\n","  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from pydicom_seg) (1.23.5)\n","Requirement already satisfied: pydicom>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from pydicom_seg) (2.4.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom_seg) (23.1.0)\n","Collecting pyrsistent>=0.14.0 (from jsonschema<4.0.0,>=3.2.0->pydicom_seg)\n","  Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom_seg) (67.7.2)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<4.0.0,>=3.2.0->pydicom_seg) (1.16.0)\n","Installing collected packages: pyrsistent, jsonschema, pydicom_seg\n","  Attempting uninstall: jsonschema\n","    Found existing installation: jsonschema 4.19.2\n","    Uninstalling jsonschema-4.19.2:\n","      Successfully uninstalled jsonschema-4.19.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed jsonschema-3.2.0 pydicom_seg-0.4.1 pyrsistent-0.20.0\n","Collecting dicom2nifti\n","  Downloading dicom2nifti-2.4.9-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from dicom2nifti) (4.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dicom2nifti) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dicom2nifti) (1.11.3)\n","Requirement already satisfied: pydicom>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from dicom2nifti) (2.4.3)\n","Collecting python-gdcm (from dicom2nifti)\n","  Downloading python_gdcm-3.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel->dicom2nifti) (23.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->dicom2nifti) (67.7.2)\n","Installing collected packages: python-gdcm, dicom2nifti\n","Successfully installed dicom2nifti-2.4.9 python-gdcm-3.0.22\n","Collecting pyradiomics\n","  Downloading pyradiomics-3.1.0.tar.gz (34.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Discarding \u001b[4;34mhttps://files.pythonhosted.org/packages/03/c1/20fc2c50ab1e3304da36d866042a1905a2b05a1431ece35448ab6b4578f2/pyradiomics-3.1.0.tar.gz (from https://pypi.org/simple/pyradiomics/)\u001b[0m: \u001b[33mRequested pyradiomics from https://files.pythonhosted.org/packages/03/c1/20fc2c50ab1e3304da36d866042a1905a2b05a1431ece35448ab6b4578f2/pyradiomics-3.1.0.tar.gz has inconsistent version: expected '3.1.0', but metadata has '3.0.1a1'\u001b[0m\n","  Downloading pyradiomics-3.0.1.tar.gz (34.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from pyradiomics) (1.23.5)\n","Requirement already satisfied: SimpleITK>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from pyradiomics) (2.3.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pyradiomics) (1.4.1)\n","Collecting pykwalify>=1.6.0 (from pyradiomics)\n","  Downloading pykwalify-1.8.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pyradiomics) (1.16.0)\n","Collecting docopt>=0.6.2 (from pykwalify>=1.6.0->pyradiomics)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from pykwalify>=1.6.0->pyradiomics) (2.8.2)\n","Collecting ruamel.yaml>=0.16.0 (from pykwalify>=1.6.0->pyradiomics)\n","  Downloading ruamel.yaml-0.18.5-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.16.0->pykwalify>=1.6.0->pyradiomics)\n","  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyradiomics, docopt\n","  Building wheel for pyradiomics (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyradiomics: filename=pyradiomics-3.0.1-cp310-cp310-linux_x86_64.whl size=169716 sha256=eaf7aaa3e401c1894c101984ab3a8b62e190f897b72f2ca1e52a069754eb89e0\n","  Stored in directory: /root/.cache/pip/wheels/91/c5/13/c5fd4c5ad3edf4062bb3855bd66fad25871c9c6dc0b3fda544\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=4eca89bdbcef6e5ec7d95a5eca6f86b69776b6605f8d0ea4c806f06b15d743a1\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","Successfully built pyradiomics docopt\n","Installing collected packages: docopt, ruamel.yaml.clib, ruamel.yaml, pykwalify, pyradiomics\n","Successfully installed docopt-0.6.2 pykwalify-1.8.0 pyradiomics-3.0.1 ruamel.yaml-0.18.5 ruamel.yaml.clib-0.2.8\n"]}]},{"cell_type":"markdown","source":["# 2. Importación de módulos"],"metadata":{"id":"YBCIurbos6YO"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"2_hb3vumJEl1","executionInfo":{"status":"ok","timestamp":1699894879897,"user_tz":-60,"elapsed":8980,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"outputs":[],"source":["#Librerías basicas\n","import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import gzip\n","import csv\n","from PIL import Image\n","import random\n","import scikeras\n","\n","#Librerías Keras y Tensorflow\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Dropout, LSTM\n","from tensorflow.keras import layers, models\n","\n","#Librería Scikit-Learn para preprocesar datos\n","import sklearn\n","from sklearn.decomposition import PCA\n","from sklearn import preprocessing\n","from sklearn.covariance import EllipticEnvelope\n","from sklearn.preprocessing import MinMaxScaler\n","\n","#Librería Scikit-Learn para crear y entrenar modelos\n","from sklearn.model_selection import train_test_split, cross_validate, KFold, cross_val_predict, cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from keras.utils import plot_model\n","from keras import regularizers\n","\n","#Librería Scikit-Learn para optimizar hiperparámetros\n","from sklearn.model_selection import GridSearchCV\n","from scikeras.wrappers import KerasClassifier\n","from sklearn.model_selection import StratifiedKFold\n","\n","#Librerías para leer imágenes médicas y extraer características radiómicas\n","import SimpleITK as sitk\n","\n","from radiomics import featureextractor\n","import radiomics\n","from radiomics import firstorder, glcm, glrlm, glszm, gldm, ngtdm, shape, shape2D\n","\n","import pywt\n","import pydicom\n","from pydicom import dcmread\n","import pyplastimatch\n","import dcm2niix\n","import pydicom_seg\n","import dicom2nifti\n","\n","#Librería importada para segmentar tumor\n","import sys\n","sys.path.insert(0,SEGMENTATION_CODE_FOLDER)\n","from TheDuneAI import ContourPilot as cp"]},{"cell_type":"markdown","source":["# 3. Funciones para preprocesar los datos"],"metadata":{"id":"B7BjOeaZGG8z"}},{"cell_type":"markdown","source":["## 3.1 Para los datos genómicos"],"metadata":{"id":"RCOa44AyJP1Y"}},{"cell_type":"code","source":["def eliminaOutliers(df):\n","\t# Usamos el algoritmo de envolvente elíptica para detectar los outliers. Eliminaremos un 10% de los valores anómalos\n","\tenveliptica = EllipticEnvelope(contamination=0.10, support_fraction=0.8, random_state=42)\n","\tdf_outliers = enveliptica.fit_predict(df)\n","\tdf = df[df_outliers==1]\n","\treturn df\n","\n","def eliminaAtributosCorrelacionados(df):\n","\tmatriz_corr = df.corr().abs()\n","\ttriang_superior = matriz_corr.where(np.triu(np.ones(matriz_corr.shape), k=1).astype(bool))\n","\tcolumnas_correlacionadas = [column for column in triang_superior.columns if any(triang_superior[column] > 0.90)]\n","\tdf.drop(columnas_correlacionadas, axis=1, inplace=True)\n","\treturn df\n","\n","def aplicaPCA(df):\n","\tindex_names = df.index.tolist()\n","\tdata_genoma = df.values\n","\tpca = PCA(n_components=0.98)\n","\tgenoma_pca = pca.fit_transform(data_genoma)\n","\tpca_df = pd.DataFrame(data=genoma_pca, index=index_names)\n","\n","\treturn pca_df"],"metadata":{"id":"24me20AGGlcA","executionInfo":{"status":"ok","timestamp":1699894879897,"user_tz":-60,"elapsed":5,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def preprocesaDatosGenomicos(path_dataset):\n","\tprint(\"Preprocesando datos genómicos\")\n","\tpath_genoma_txt = os.path.join(path_dataset, 'GSE103584_R01_NSCLC_RNAseq.txt.gz')\n","\tdf_genoma = pd.read_csv(gzip.open(path_genoma_txt), sep='\\t', index_col=0).fillna(0)\n","\tdf_genoma = (df_genoma.T)\n","\t#---------- PREPROCESADO DE LOS DATOS ----------#\n","\t#---------- 1. Estimación de valores ausentes ----------#\n","\t#Sustituimos los valores ausentes por la media\n","\tdf_genoma.fillna(df_genoma.mean(), inplace=True)\n","\t#---------- 2. Normalización ----------#\n","\t#Normalizamos los datos\n","\tscaler = preprocessing.MinMaxScaler()\n","\tpacientes_id = df_genoma.index.values\n","\tdf_genoma = pd.DataFrame(scaler.fit_transform(df_genoma),columns = df_genoma.columns, index = pacientes_id)\n","\n","\t#---------- 3. Reducción de dimensionalidad ----------#\n","\t#Reducimos el número de columnas para evitar descompensar el dataset final\n","\tdisplay(df_genoma)\n","\tdf_genoma = aplicaPCA(df_genoma)\n","\n","\t#---------- 4. Tratamiento de outliers ------------#\n","\tdisplay(df_genoma)\n","\tdf_genoma = eliminaOutliers(df_genoma)\n","\tdisplay(df_genoma)\n","\treturn df_genoma"],"metadata":{"id":"yrm-smkYLe06","executionInfo":{"status":"ok","timestamp":1699894879897,"user_tz":-60,"elapsed":4,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 Para los los datos clínicos"],"metadata":{"id":"hvdwPtHmJH95"}},{"cell_type":"code","source":["def preprocesarFechas (df_pacientes):\n","\t#Para las fechas eliminamos los labels Not Collected y las convertimos a valor numérico timestamp\n","\t#Para Date of Recurrence\n","\tdf_pacientes[\"Date of Recurrence\"].replace(\"Not Collected\",np.nan, inplace=True)\n","\tdf_pacientes[\"Date of Recurrence\"].fillna(\"5/5/1995\", inplace=True)\n","\tdf_pacientes[\"Date of Recurrence\"] = pd.to_datetime(df_pacientes['Date of Recurrence']).astype(int)\n","\n","\t#Para CT Date\n","\tdf_pacientes[\"CT Date\"].replace(\"Not Collected\",np.nan, inplace=True)\n","\tdf_pacientes[\"CT Date\"].fillna(\"5/5/1995\", inplace=True)\n","\tdf_pacientes[\"CT Date\"] = pd.to_datetime(df_pacientes['CT Date']).astype(int)\n","\n","\t#Para PET Date\n","\tdf_pacientes[\"PET Date\"].replace(\"Not Collected\",np.nan, inplace=True)\n","\tdf_pacientes[\"PET Date\"].fillna(\"5/5/1995\", inplace=True)\n","\tdf_pacientes[\"PET Date\"] = pd.to_datetime(df_pacientes['PET Date']).astype(int)\n","\n","\t#Para Date of Death\n","\tdf_pacientes[\"Date of Death\"].replace(\"Not Collected\",np.nan, inplace=True)\n","\tdf_pacientes[\"Date of Death\"].fillna(\"5/5/1995\", inplace=True)\n","\tdf_pacientes[\"Date of Death\"] = pd.to_datetime(df_pacientes['Date of Death']).astype(int)\n","\n","\t#Para Date of Last Known Alive\n","\tdf_pacientes[\"Date of Last Known Alive\"].replace(\"Not Collected\",np.nan, inplace=True)\n","\tdf_pacientes[\"Date of Last Known Alive\"].fillna(\"5/5/1995\", inplace=True)\n","\tdf_pacientes[\"Date of Last Known Alive\"] = pd.to_datetime(df_pacientes['Date of Last Known Alive']).astype(int)\n","\treturn df_pacientes\n","\n","def preprocesaDatosClinicos(path_dataset):\n","\tprint(\"Preprocesando datos clínicos\")\n","\tpath_csv = os.path.join(path_dataset, 'NSCLCR01Radiogenomic_DATA_LABELS_2018-05-22_1500-shifted.csv')\n","\tdf_pacientes = pd.read_csv(path_csv)\n","\tdf_pacientes.set_index('Case ID', inplace=True)\n","\tdisplay(df_pacientes)\n","\n","\t#----------PREPROCESADO DE LOS DATOS----------#\n","\t#Los valores númericos que estan en String los convertimos a números\n","\tdf_pacientes[\"Weight (lbs)\"] = df_pacientes[\"Weight (lbs)\"].apply(pd.to_numeric, errors = 'coerce')\n","\tdf_pacientes[\"Pack Years\"] = df_pacientes[\"Pack Years\"].apply(pd.to_numeric, errors = 'coerce')\n","\n","\t#Preprocesamos las fechas\n","\tdf_pacientes = preprocesarFechas(df_pacientes)\n","\n","\t#3.Codificamos las variables categóricas en números y normalizamos las variables numéricas\n","\tfor column in df_pacientes:\n","\t\tif (df_pacientes[column].dtype == np.float64) or (df_pacientes[column].dtype == np.int64):\n","\t\t\t#Sustituimos los valores NaN (valores ausentes) por la media\n","\t\t\tdf_pacientes[column].fillna(df_pacientes[column].mean(), inplace=True)\n","\t\t\tdf_pacientes[column] = (df_pacientes[column] - df_pacientes[column].min())/(df_pacientes[column].max() - df_pacientes[column].min())\n","\t\telse:\n","\t\t\tdf_pacientes = pd.get_dummies(data=df_pacientes, columns=[column], prefix=[column], prefix_sep= \" | \")\n","\t#Elimanos atributos correlacionados\n","\tdf_pacientes = eliminaAtributosCorrelacionados(df_pacientes)\n","\t#Relacionamos las datos genómicos con los clínicos con un join\n","\tdf_pacientes.fillna(df_pacientes.mean(), inplace=True)\n","\tdisplay(df_pacientes)\n","\n","\treturn df_pacientes"],"metadata":{"id":"THv1yszzJc_7","executionInfo":{"status":"ok","timestamp":1699894879897,"user_tz":-60,"elapsed":4,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## 3.3. Para los datos radiómicos"],"metadata":{"id":"4tyEr1bEJrNI"}},{"cell_type":"markdown","source":["### 3.3.1 Funciones para guardar las imágenes en NRRD"],"metadata":{"id":"UIs-4LcbJ_In"}},{"cell_type":"code","source":["def contiene_valores(group):\n","    return ('CT' in group['Modality'].values) and ('SEG' in group['Modality'].values)\n","\n","def obtenPathCorrecto(path):\n","\tstring_nslsc = r\".\\NSCLC Radiogenomics\" + \"\\\\\"\n","\tpath_correcto = path.replace(string_nslsc, '').replace(\"\\\\\",\"/\")\n","\treturn path_correcto\n","\n","def escribirCTNRRD(subject_id, path_nttd, path_ct_original):\n","    path_ct_original = os.path.join(IMAGES_FOLDER, path_ct_original)\n","    path_subject_nrrd = os.path.join(path_nttd, subject_id)\n","\n","    if not os.path.exists(path_subject_nrrd):\n","        os.makedirs(path_subject_nrrd)\n","\n","    path_ct_nrrd = os.path.join(path_subject_nrrd, \"image.nrrd\")\n","    if not os.path.exists(path_ct_nrrd):\n","        reader = sitk.ImageSeriesReader()\n","        dicom_names = reader.GetGDCMSeriesFileNames(path_ct_original)\n","        reader.SetFileNames(dicom_names)\n","        dicom_image = reader.Execute()\n","        sitk.WriteImage(dicom_image, path_ct_nrrd)\n","\n","def escribirSegmentacionNRRD(subject_id, path_nttd, path_seg_original):\n","    path_seg_original = os.path.join(IMAGES_FOLDER, path_seg_original)\n","    archivo_seg = os.listdir(path_seg_original)[0]\n","    path_seg_original = os.path.join(path_seg_original, archivo_seg)\n","\n","    path_subject_nrrd = os.path.join(path_nttd, subject_id)\n","    if not os.path.exists(path_subject_nrrd):\n","        os.makedirs(path_subject_nrrd)\n","\n","    path_seg_nrrd = os.path.join(path_subject_nrrd, \"mask.nrrd\")\n","    if not os.path.exists(path_seg_nrrd):\n","        dcm_imagen_seg = pydicom.dcmread(path_seg_original)\n","\n","        reader = pydicom_seg.MultiClassReader()\n","        result = reader.read(dcm_imagen_seg)\n","\n","        image_data = result.data\n","        image = result.image\n","        sitk.WriteImage(image, path_seg_nrrd, True)\n","\n","def guardarImagenesConSegmentacionNRRD(path_dataset):\n","\tpath_imagenes = os.path.join(path_dataset, 'Imagenes')\n","\n","\tdf_metadata = pd.read_csv(os.path.join(path_dataset, 'metadata.csv'))\n","\tdf_metadata = df_metadata.reset_index()\n","\n","\tfor subject_id in df_metadata['Subject ID'].unique():\n","\t\tdf_filtered_by_subject = df_metadata[df_metadata['Subject ID'] == subject_id]\n","\t\tdf_grouped_by_subject = df_filtered_by_subject.groupby('Subject ID')\n","\n","\t\tfiltered = df_grouped_by_subject.filter(contiene_valores)\n","\t\tif not filtered.empty:\n","\t\t\tprint(\"------ \" + subject_id + \" ------\")\n","\t\t\t#Filtramos el dataframe para solo obtener las filas que sean segmentaciones\n","\t\t\trow_where_seg = df_filtered_by_subject[df_filtered_by_subject['Modality'] == 'SEG']\n","            #Obtenemos el valor del path a esa segmentacion\n","\t\t\tpath_mask = row_where_seg['File Location'].values[0]\n","\t\t\tpath_carpeta_general = path_mask.rsplit('\\\\', 1)[0]\n","\n","            #Obtenemos el path a la imagen ct de la segmentación\n","\t\t\trows_where_ct = df_filtered_by_subject[df_filtered_by_subject['Modality'] == 'CT']\n","\t\t\trow_where_ct = rows_where_ct[rows_where_ct['File Location'].str.startswith(path_carpeta_general)]\n","\t\t\tpath_ct = row_where_ct['File Location'].values[0]\n","\n","\t\t\tescribirCTNRRD(subject_id, IMAGES_NRRD_FOLDER, obtenPathCorrecto(path_ct))\n","\t\t\tescribirSegmentacionNRRD(subject_id, IMAGES_NRRD_FOLDER, obtenPathCorrecto(path_mask))\n","\n","def guardarImagenesSinSegmentacionNRRD(path_dataset):\n","\tpath_imagenes = os.path.join(path_dataset, 'Imagenes')\n","\n","\tdf_metadata = pd.read_csv(os.path.join(path_dataset, 'metadata.csv'))\n","\tdf_metadata = df_metadata.reset_index()\n","\n","\tfor subject_id in df_metadata['Subject ID'].unique():\n","\t\tdf_filtered_by_subject = df_metadata[df_metadata['Subject ID'] == subject_id]\n","\t\tdf_grouped_by_subject = df_filtered_by_subject.groupby('Subject ID')\n","\n","\t\tfiltered = df_grouped_by_subject.filter(contiene_valores)\n","\t\tif filtered.empty:\n","\t\t\tprint(\"NOT SEG ------ \" + subject_id + \" ------\")\n","\n","\t\t\trows_where_ct = df_filtered_by_subject[df_filtered_by_subject['Modality'] == 'CT']\n","\t\t\trows_with_no_pet = rows_where_ct[~rows_where_ct['File Location'].str.contains('PET')]\n","\t\t\tmax_num_files = rows_with_no_pet['Number of Images'].max()\n","\t\t\trow_max_num_files = rows_with_no_pet[rows_with_no_pet['Number of Images'] == max_num_files]\n","\t\t\tnum_rows = row_max_num_files.shape\n","\t\t\tnum_rows, num_columns = row_max_num_files.shape\n","\t\t\tif num_rows > 0:\n","\t\t\t\tpath_ct = row_max_num_files['File Location'].values[0]\n","\t\t\t\tescribirCTNRRD(subject_id, IMAGES_PRED_NRRD_FOLDER, obtenPathCorrecto(path_ct))\n"],"metadata":{"id":"He--nsNIJ76Y","executionInfo":{"status":"ok","timestamp":1699894879897,"user_tz":-60,"elapsed":4,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### 3.3.2 Funciones para predecir las segmentaciones restantes"],"metadata":{"id":"EMYhtfUXKTiU"}},{"cell_type":"code","source":["def predecirSegmentacionNRRD(path_dataset):\n","\tpath_imagenes = os.path.join(path_dataset, 'Imagenes')\n","\tpath_modelo = os.path.join(SEGMENTATION_CODE_FOLDER, 'Modelo')\n","\tmodel = cp(path_modelo,IMAGES_PRED_NRRD_FOLDER,IMAGES_PRED_NRRD_FOLDER,verbosity=True)\n","\tmodel.segment()\n","\n","def guardarImagenesYPredecirSegmentacionNRRD(path_dataset):\n","\tguardarImagenesSinSegmentacionNRRD(path_dataset)\n","\tguardarImagenesConSegmentacionNRRD(path_dataset)\n","\tpredecirSegmentacionNRRD(path_dataset)"],"metadata":{"id":"EGgTT1mcKZER","executionInfo":{"status":"ok","timestamp":1699894879897,"user_tz":-60,"elapsed":4,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### 3.3.3 Funciones para extrar y guardar los datos radiómicos"],"metadata":{"id":"c5w56kK9K7uP"}},{"cell_type":"code","source":["def extraerDatosRadiomicosImagen(extractor, path_nrrd):\n","    path_nrrd_ct = os.path.join(path_nrrd, 'image.nrrd')\n","    path_nrrd_seg = os.path.join(path_nrrd, 'mask.nrrd')\n","\n","    image_nrrd_ct = sitk.ReadImage(path_nrrd_ct)\n","    image_nrrd_seg = sitk.ReadImage(path_nrrd_seg)\n","    try:\n","        firstOrderFeatures = firstorder.RadiomicsFirstOrder(image_nrrd_ct, image_nrrd_seg)\n","        glcmFeatures = glcm.RadiomicsGLCM(image_nrrd_ct, image_nrrd_seg)\n","        glrlmFeatures = glrlm.RadiomicsGLRLM(image_nrrd_ct, image_nrrd_seg)\n","        ngtdmFeatures = ngtdm.RadiomicsNGTDM(image_nrrd_ct, image_nrrd_seg)\n","        gldmFeatures = gldm.RadiomicsGLDM(image_nrrd_ct, image_nrrd_seg)\n","    except Exception as e:\n","        return np.empty(0)\n","    else:\n","        firstOrderFeatures = np.array([v for _, v in firstOrderFeatures.execute().items()])\n","        glcmFeatures = np.array([v for _, v in glcmFeatures.execute().items()])\n","        glrlmFeatures = np.array([v for _, v in glrlmFeatures.execute().items()])\n","        ngtdmFeatures = np.array([v for _, v in ngtdmFeatures.execute().items()])\n","        gldmFeatures = np.array([v for _, v in gldmFeatures.execute().items()])\n","        caract_radiomics = np.hstack((firstOrderFeatures, glcmFeatures, glrlmFeatures, ngtdmFeatures, gldmFeatures))\n","        return caract_radiomics\n","\n","def extraerDatosRadiomicosCarpeta(path_carpeta, path_caract_radiomicas_parciales):\n","    lista_caract_radiomicas = {}\n","    lista_sujetos = {}\n","    if os.path.exists(path_caract_radiomicas_parciales):\n","        df_radiomicas_parciales = pd.read_csv(path_caract_radiomicas_parciales, index_col=0, header=0)\n","        for index, row in df_radiomicas_parciales.iterrows():\n","            lista_caract_radiomicas[index] = row.values\n","        lista_sujetos = df_radiomicas_parciales.index.tolist()\n","\n","    extractor = featureextractor.RadiomicsFeatureExtractor()\n","\n","    for nombre_subcarpeta in os.listdir(path_carpeta):\n","        if nombre_subcarpeta not in lista_sujetos:\n","            print(\"   paciente \" + nombre_subcarpeta)\n","            path_subcarpeta= os.path.join(path_carpeta, nombre_subcarpeta)\n","            datos_radiomicos_imagen = extraerDatosRadiomicosImagen(extractor, path_subcarpeta)\n","\n","            if datos_radiomicos_imagen.size != 0:\n","                lista_caract_radiomicas[nombre_subcarpeta] = datos_radiomicos_imagen\n","                df_radiomicas_parciales = pd.DataFrame.from_dict(lista_caract_radiomicas, orient='index')\n","                df_radiomicas_parciales.fillna(0, inplace=True)\n","                df_radiomicas_parciales.index.name='Case ID'\n","                df_radiomicas_parciales.to_csv(path_caract_radiomicas_parciales, index=True, header=True)\n","    return lista_caract_radiomicas\n","\n","def procesaDatosRadiomicos(lista_caract_radiomicas):\n","    df_imagenes = pd.DataFrame.from_dict(lista_caract_radiomicas, orient='index')\n","    df_imagenes.fillna(0, inplace=True)\n","    scaler = preprocessing.MinMaxScaler()\n","    pacientes_id = df_imagenes.index.values\n","    caract_imagenes = pd.DataFrame(scaler.fit_transform(df_imagenes),columns = df_imagenes.columns, index = pacientes_id)\n","    caract_imagenes.index.name='Case ID'\n","    return caract_imagenes\n","\n","def extraerDatosRadiomicos(path_dataset):\n","    print(\"Extrayendo datos radiómicos...\")\n","\n","    path_caract_radiomicas_parciales = os.path.join(path_dataset, 'Caracteristicas_extraidas/datos_radiomicos_parciales.csv')\n","\n","    lista_caract_radiomicas_seg_orig = extraerDatosRadiomicosCarpeta(IMAGES_NRRD_FOLDER, path_caract_radiomicas_parciales)\n","    lista_caract_radiomicas = extraerDatosRadiomicosCarpeta(IMAGES_PRED_NRRD_FOLDER, path_caract_radiomicas_parciales)\n","\n","    return procesaDatosRadiomicos(lista_caract_radiomicas)\n"],"metadata":{"id":"04-2W2uvLBpk","executionInfo":{"status":"ok","timestamp":1699894879897,"user_tz":-60,"elapsed":4,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# 4. Extracción de las características"],"metadata":{"id":"GdKaWyW8XWYM"}},{"cell_type":"markdown","source":["## 4.1 Guardar características"],"metadata":{"id":"EcBZvA07l5N5"}},{"cell_type":"code","source":["def preprocesaYGuardaCaracteristicas(path_dataset):\n","    #Si las características de los datos clínicos no habían sido extraidas y guardadas, las guardamos\n","    path_caract_clinicas = os.path.join(path_dataset, 'Caracteristicas_extraidas/datos_clinicos.csv')\n","    if not os.path.isfile(path_caract_clinicas):\n","        caract_clinicas = preprocesaDatosClinicos(path_dataset)\n","        caract_clinicas.to_csv(path_caract_clinicas, index=True, header=True)\n","\n","    #Si las características de los datos genómicos no habían sido extraidas y guardadas, las guardamos\n","    path_caract_genomicas = os.path.join(path_dataset, 'Caracteristicas_extraidas/datos_genomicos.csv')\n","    if not os.path.isfile(path_caract_genomicas):\n","        caract_genomicas = preprocesaDatosGenomicos(path_dataset)\n","        caract_genomicas.to_csv(path_caract_genomicas, index=True, header=True)\n","\n","    #Si las características de los datos radiómicos no habían sido extraidas y guardadas, las guardamos\n","    path_caract_radiomicas = os.path.join(path_dataset, 'Caracteristicas_extraidas/datos_radiomicos.csv')\n","    if not os.path.isfile(path_caract_radiomicas):\n","        caract_radiomicas = extraerDatosRadiomicos(path_dataset)\n","        caract_radiomicas.to_csv(path_caract_radiomicas, index=True, header=True)"],"metadata":{"id":"nHLwmhpJXn9M","executionInfo":{"status":"ok","timestamp":1699894879897,"user_tz":-60,"elapsed":4,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## 4.2 Extraer características"],"metadata":{"id":"r_84VNaul-mp"}},{"cell_type":"code","source":["def extraeCaracteristicasCompletas(path_caract_1, path_caract_2, path_caract_3):\n","    caract_1 = pd.read_csv(path_caract_1, index_col=0, header=0)\n","    caract_2 = pd.read_csv(path_caract_2, index_col=0, header=0)\n","    caract_3 = pd.read_csv(path_caract_3, index_col=0, header=0)\n","\n","    #Concatenamos las características\n","    caract_merged = pd.merge(caract_1, caract_2, left_index=True, right_index=True, how=\"outer\")\n","    caract = pd.merge(caract_merged, caract_3, left_index=True, right_index=True, how=\"outer\")\n","    caract.fillna(caract.mean(), inplace=True)\n","    return caract\n","\n","def extraeDosCaracteristicas(es_clinica, path_caract_1, path_caract_2):\n","    caract_1 = pd.read_csv(path_caract_1, index_col=0, header=0)\n","    caract_2 = pd.read_csv(path_caract_2, index_col=0, header=0)\n","\n","    #Concatenamos las características\n","    caract = pd.merge(caract_1, caract_2, left_index=True, right_index=True, how=\"outer\")\n","    caract.fillna(caract.mean(), inplace=True)\n","    if not es_clinica:\n","        path_caract_clinicas = path_caract_clinicas = os.path.join(DATASET_FOLDER, 'Caracteristicas_extraidas/datos_clinicos.csv')\n","        caract_clinicas = pd.read_csv(path_caract_clinicas, index_col=0, header=0)\n","        survival_status_col = caract_clinicas['Survival Status | Alive']\n","        df_survival = survival_status_col.to_frame(name='Survival Status | Alive')\n","        caract = pd.merge(caract, df_survival, left_index=True, right_index=True, how=\"inner\")\n","    return caract\n","\n","def extraeUnaCaracteristica(es_clinica, path_caract):\n","    caract = pd.read_csv(path_caract, index_col=0, header=0)\n","    if not es_clinica:\n","        path_caract_clinicas = os.path.join(DATASET_FOLDER, 'Caracteristicas_extraidas/datos_clinicos.csv')\n","        caract_clinicas = pd.read_csv(path_caract_clinicas, index_col=0, header=0)\n","        survival_status_col = caract_clinicas['Survival Status | Alive']\n","        df_survival = survival_status_col.to_frame(name='Survival Status | Alive')\n","        caract = pd.merge(caract, df_survival, left_index=True, right_index=True, how=\"inner\")\n","    return caract\n","\n","def extraeCaractSegunInput(clinicos, genomicos, radiomicos):\n","    path_caract_clinicas = os.path.join(DATASET_FOLDER, 'Caracteristicas_extraidas/datos_clinicos.csv')\n","    path_caract_genomicas = os.path.join(DATASET_FOLDER, 'Caracteristicas_extraidas/datos_genomicos.csv')\n","    path_caract_radiomicas = os.path.join(DATASET_FOLDER, 'Caracteristicas_extraidas/datos_radiomicos.csv')\n","    df_caract = {}\n","    if clinicos and genomicos and radiomicos:\n","        df_caract = extraeCaracteristicasCompletas(path_caract_clinicas, path_caract_genomicas, path_caract_radiomicas)\n","    elif clinicos and genomicos and not radiomicos:\n","        df_caract = extraeDosCaracteristicas(clinicos, path_caract_clinicas, path_caract_genomicas)\n","    elif clinicos and not genomicos and radiomicos:\n","        df_caract = extraeDosCaracteristicas(clinicos, path_caract_clinicas, path_caract_radiomicas)\n","    elif not clinicos and genomicos and radiomicos:\n","        df_caract = extraeDosCaracteristicas(clinicos, path_caract_genomicas, path_caract_radiomicas)\n","    elif clinicos and not genomicos and not radiomicos:\n","        df_caract = extraeUnaCaracteristica(clinicos, path_caract_clinicas)\n","    elif not clinicos and genomicos and not radiomicos:\n","        df_caract = extraeUnaCaracteristica(clinicos, path_caract_genomicas)\n","    elif not clinicos and not genomicos and radiomicos:\n","        df_caract = extraeUnaCaracteristica(clinicos, path_caract_radiomicas)\n","    df_caract = df_caract._get_numeric_data()\n","    return df_caract"],"metadata":{"id":"xHxa16lYl1o_","executionInfo":{"status":"ok","timestamp":1699894879898,"user_tz":-60,"elapsed":5,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# 5. Implementación modelo"],"metadata":{"id":"daBXB34ifIgj"}},{"cell_type":"markdown","source":["## 5.1 Funciones auxiliares"],"metadata":{"id":"WuPSIgZ5e_kX"}},{"cell_type":"code","source":["#Función para mostrar curva de aprendizaje\n","def mostrarCurvaAprendizaje(H, num_epocas):\n","    plt.style.use(\"ggplot\")\n","    plt.figure()\n","    #plt.plot(range(num_epocas), H.history[\"loss\"], label=\"train_loss\")\n","    #plt.plot(range(num_epocas), H.history[\"val_loss\"], label=\"val_loss\")\n","    plt.plot(range(num_epocas), H.history[\"accuracy\"], label=\"train_acc\")\n","    plt.plot(range(num_epocas), H.history[\"val_accuracy\"], label=\"val_acc\")\n","    plt.title(\"Training Loss and Accuracy\")\n","    plt.xlabel(\"Epoch #\")\n","    plt.ylabel(\"Loss/Accuracy\")\n","    plt.legend()\n","    plt.show()\n","\n","#Función para obtener y mostrar métricas\n","def obtenMetricas(y_test, y_pred):\n","    binary_predictions = (y_pred > 0.5).astype(int)\n","\n","    accuracy = accuracy_score(y_test, binary_predictions)\n","    precision = precision_score(y_test, binary_predictions)\n","    recall = recall_score(y_test, binary_predictions)\n","    f1 = f1_score(y_test, binary_predictions)\n","    print('Métricas: {')\n","    print('     Accuracy:  %.4f' % accuracy)\n","    print('     Precision: %.4f' % precision)\n","    print('     Recall: %.4f' % recall)\n","    print('     F1:   %.4f' % f1)\n","    print('}')\n","\n","#Función para optimizar los hiperparámetros del modelo de red neuronal creado\n","def optimizaHiperparametros(x_train, y_train):\n","    input_dim = x_train.shape[1]\n","    modelo = KerasClassifier(build_fn=creaModeloMultimodal, input_dim=input_dim, epochs=10, batch_size=32, dropout_rate=0.2,\n","                             neuronas_capa_1=512, neuronas_capa_2=512, neuronas_capa_3=256, learning_rate=0.001, verbose=0)\n","    parametros = {\n","        'neuronas_capa_1': [256, 512],\n","        'neuronas_capa_2': [256, 512],\n","        'neuronas_capa_3': [128, 256],\n","        'dropout_rate': [0.2, 0.3],\n","        'learning_rate': [0.001, 0.01],\n","        'batch_size': [16, 32],\n","        'epochs': [60, 80]\n","    }\n","    #Usamos la validación cruzada\n","    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    grid = GridSearchCV(estimator=modelo, param_grid=parametros, cv=kfold, scoring='accuracy')\n","    grid_result = grid.fit(x_train, y_train)\n","    print(\"Mejores parámetros encontrados: \", grid_result.best_params_)\n","    print(\"Mejor precisión encontrada: \", grid_result.best_score_)\n","\n","def defineHiperparametrosSegunDatos():\n","    param_datos = {}\n","    param_datos[\"Datos clínicos\"] = {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 60, 'learning_rate': 0.01, 'neuronas_capa_1': 256, 'neuronas_capa_2': 256, 'neuronas_capa_3': 256}\n","    param_datos[\"Datos genómicos\"] = {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 80, 'learning_rate': 0.001, 'neuronas_capa_1': 256, 'neuronas_capa_2': 256, 'neuronas_capa_3': 128}\n","    param_datos[\"Datos radiómicos\"] = {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 80, 'learning_rate': 0.01, 'neuronas_capa_1': 256, 'neuronas_capa_2': 256, 'neuronas_capa_3': 256}\n","    param_datos[\"Datos clínicos y genómicos\"] = {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 60, 'learning_rate': 0.001, 'neuronas_capa_1': 256, 'neuronas_capa_2': 512, 'neuronas_capa_3': 128}\n","    param_datos[\"Datos clínicos y radiómicos\"] = {'batch_size': 16, 'dropout_rate': 0.2, 'epochs': 80, 'learning_rate': 0.01, 'neuronas_capa_1': 512, 'neuronas_capa_2': 256, 'neuronas_capa_3': 128}\n","    param_datos[\"Datos genómicos y radiómicos\"] = {'batch_size': 16, 'dropout_rate': 0.3, 'epochs': 80, 'learning_rate': 0.01, 'neuronas_capa_1': 512, 'neuronas_capa_2': 256, 'neuronas_capa_3': 256}\n","    param_datos[\"Datos clínicos, genómicos y radiómicos\"] = {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 60, 'learning_rate': 0.01, 'neuronas_capa_1': 512, 'neuronas_capa_2': 512, 'neuronas_capa_3': 256}\n","    return param_datos\n","\n","def obtenMetricasValTrainRedNeuronal(modelo, num_epocas, batch_size, x_train, x_val, y_train, y_val):\n","    H = modelo.fit(x_train, y_train, epochs=num_epocas, batch_size=batch_size, validation_data=(x_val, y_val), verbose=0)\n","    last_validation_accuracy = H.history['val_accuracy'][-1]\n","    last_train_accuracy = H.history['accuracy'][-1]\n","    print(\"Val_accuracy:\", last_validation_accuracy)\n","    print(\"Train_accuracy:\", last_train_accuracy)\n","\n","def obtenerPrecisionValTrainConjuntoModelos(x, y, comb_datos, objetivo):\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)\n","    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state=42)\n","    conjunto_modelos = {}\n","\n","    input_dim = x_train.shape[1]\n","    conjunto_modelos[\"Red neuronal propia\"] = creaModeloMultimodal(input_dim, comb_datos['neuronas_capa_1'], comb_datos['neuronas_capa_2'], comb_datos['neuronas_capa_3'], comb_datos['dropout_rate'], comb_datos['learning_rate'])\n","    conjunto_modelos[\"Clasificador MLP\"] = MLPClassifier(solver='adam', random_state=42)\n","    conjunto_modelos[\"Regresión logística\"] = LogisticRegression(random_state=42)\n","    conjunto_modelos[\"K vecinos\"] = KNeighborsClassifier(n_neighbors=6,  metric=\"euclidean\")\n","    print(\"-----------------------\")\n","    for i, modelo in conjunto_modelos.items():\n","        print(\"Modelo:  \" + i)\n","        if i == \"Red neuronal propia\":\n","            y_pred = obtenMetricasValTrainRedNeuronal(modelo, comb_datos['epochs'], comb_datos['batch_size'], x_train, x_val, y_train, y_val)\n","        else:\n","            #Creamos 5 paquetes para la validación cruzada\n","            kf = KFold(n_splits=5)\n","            modelo = modelo.fit(x_train, y_train)\n","            val_scores = cross_val_score(modelo, x_train, y_train, cv=kf, scoring=\"accuracy\")\n","            print(\"Val_accuracy:\", val_scores.mean())\n","\n","            y_train_pred = modelo.predict(x_train)\n","            accuracy = accuracy_score(y_train, y_train_pred)\n","            print('Test accuracy:  %.4f' % accuracy)\n","        print(\"\\n\")\n","    print(\"-----------------------\\n\")"],"metadata":{"id":"5HyeHdBEe7Gs","executionInfo":{"status":"ok","timestamp":1699899898889,"user_tz":-60,"elapsed":828,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["## 5.2 Funciones para crear modelos y obtener métricas"],"metadata":{"id":"AoCb9ad1ymaC"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","def creaModeloMultimodal(input_dim, neuronas_capa_1, neuronas_capa_2, neuronas_capa_3, dropout_rate, learning_rate):\n","    model = Sequential()\n","    model.add(Dense(neuronas_capa_1, input_dim=input_dim, activation=\"relu\"))\n","    model.add(Dense(neuronas_capa_2, activation=\"relu\"))\n","    model.add(Dense(neuronas_capa_3, activation=\"relu\"))\n","    model.add(Dropout(dropout_rate))\n","    model.add(Dense(1, activation=\"sigmoid\"))\n","\n","    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","    return model\n","\n","def prediceSupervivenciaRedNeuronal(modelo, num_epocas, batch_size, x_train, x_val, y_train, y_val, x_test):\n","    H = modelo.fit(x_train, y_train, epochs=num_epocas, batch_size=batch_size, validation_data=(x_val, y_val), verbose=0)\n","    predictions = modelo.predict(x_test, verbose=0)\n","    return predictions\n","\n","def predecirSupervivenciaConjuntoModelos(x, y, comb_datos):\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)\n","    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state=42)\n","    conjunto_modelos = {}\n","\n","    input_dim = x_train.shape[1]\n","    conjunto_modelos[\"Red neuronal propia\"] = creaModeloMultimodal(input_dim, comb_datos['neuronas_capa_1'], comb_datos['neuronas_capa_2'], comb_datos['neuronas_capa_3'], comb_datos['dropout_rate'], comb_datos['learning_rate'])\n","    conjunto_modelos[\"Clasificador MLP\"] = MLPClassifier(solver='adam', random_state=42)\n","    conjunto_modelos[\"Regresión logística\"] = LogisticRegression(random_state=42)\n","    conjunto_modelos[\"K vecinos\"] = KNeighborsClassifier(n_neighbors=6,  metric=\"euclidean\")\n","    print(\"-----------------------\")\n","    for i, modelo in conjunto_modelos.items():\n","        print(\"Modelo:  \" + i)\n","        if i == \"Red neuronal propia\":\n","            y_pred = prediceSupervivenciaRedNeuronal(modelo, comb_datos['epochs'], comb_datos['batch_size'], x_train, x_val, y_train, y_val, x_test)\n","        else:\n","            modelo = modelo.fit(x_train, y_train)\n","            y_pred = modelo.predict(x_test)\n","        obtenMetricas(y_test, y_pred)\n","    print(\"-----------------------\\n\")\n","\n","def muestraCombinacionesDatosModelos():\n","    lista_name_comb_datos = [\"Datos clínicos\", \"Datos genómicos\", \"Datos radiómicos\", \"Datos clínicos y genómicos\",\n","                             \"Datos clínicos y radiómicos\", \"Datos genómicos y radiómicos\", \"Datos clínicos, genómicos y radiómicos\"]\n","    lista_bool_comb_datos = [[True, False, False], [False, True, False], [False, False, True], [True, True, False],\n","     [True, False, True], [False, True, True], [True, True, True]]\n","    comb_datos = defineHiperparametrosSegunDatos()\n","    for i, comb_bool_datos in enumerate(lista_bool_comb_datos):\n","        df_caract = extraeCaractSegunInput(clinicos=comb_bool_datos[0], genomicos=comb_bool_datos[1], radiomicos=comb_bool_datos[2])\n","        y = df_caract.pop('Survival Status | Alive').values.astype(int)\n","        x = df_caract.values.astype(int)\n","        print(\"Tipos de datos: \", lista_name_comb_datos[i])\n","        #print(\"Tamaño datos: \", x.shape)\n","        predecirSupervivenciaConjuntoModelos(x, y, comb_datos[lista_name_comb_datos[i]])\n","\n","muestraCombinacionesDatosModelos()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YozrI2kXfMvv","executionInfo":{"status":"ok","timestamp":1699902211283,"user_tz":-60,"elapsed":36631,"user":{"displayName":"Roge Sansaloni Sanjuan","userId":"15901799583746821872"}},"outputId":"4c080861-aaf6-4b43-f2f7-5427b2f57712"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Tipos de datos:  Datos clínicos\n","-----------------------\n","Modelo:  Red neuronal propia\n","Métricas: {\n","     Accuracy:  0.6279\n","     Precision: 0.7273\n","     Recall: 0.7742\n","     F1:   0.7500\n","}\n","Modelo:  Clasificador MLP\n","Métricas: {\n","     Accuracy:  0.6279\n","     Precision: 0.7419\n","     Recall: 0.7419\n","     F1:   0.7419\n","}\n","Modelo:  Regresión logística\n","Métricas: {\n","     Accuracy:  0.6977\n","     Precision: 0.7647\n","     Recall: 0.8387\n","     F1:   0.8000\n","}\n","Modelo:  K vecinos\n","Métricas: {\n","     Accuracy:  0.6047\n","     Precision: 0.7333\n","     Recall: 0.7097\n","     F1:   0.7213\n","}\n","-----------------------\n","\n","Tipos de datos:  Datos genómicos\n","-----------------------\n","Modelo:  Red neuronal propia\n","Métricas: {\n","     Accuracy:  0.7273\n","     Precision: 0.7059\n","     Recall: 0.9231\n","     F1:   0.8000\n","}\n","Modelo:  Clasificador MLP\n","Métricas: {\n","     Accuracy:  0.5909\n","     Precision: 0.6250\n","     Recall: 0.7692\n","     F1:   0.6897\n","}\n","Modelo:  Regresión logística\n","Métricas: {\n","     Accuracy:  0.7727\n","     Precision: 0.7500\n","     Recall: 0.9231\n","     F1:   0.8276\n","}\n","Modelo:  K vecinos\n","Métricas: {\n","     Accuracy:  0.4091\n","     Precision: 0.5000\n","     Recall: 0.2308\n","     F1:   0.3158\n","}\n","-----------------------\n","\n","Tipos de datos:  Datos radiómicos\n","-----------------------\n","Modelo:  Red neuronal propia\n","Métricas: {\n","     Accuracy:  0.5556\n","     Precision: 0.5556\n","     Recall: 1.0000\n","     F1:   0.7143\n","}\n","Modelo:  Clasificador MLP\n","Métricas: {\n","     Accuracy:  0.4815\n","     Precision: 0.5217\n","     Recall: 0.8000\n","     F1:   0.6316\n","}\n","Modelo:  Regresión logística\n","Métricas: {\n","     Accuracy:  0.5556\n","     Precision: 0.5556\n","     Recall: 1.0000\n","     F1:   0.7143\n","}\n","Modelo:  K vecinos\n","Métricas: {\n","     Accuracy:  0.5926\n","     Precision: 0.6000\n","     Recall: 0.8000\n","     F1:   0.6857\n","}\n","-----------------------\n","\n","Tipos de datos:  Datos clínicos y genómicos\n","-----------------------\n","Modelo:  Red neuronal propia\n","Métricas: {\n","     Accuracy:  0.6047\n","     Precision: 0.7500\n","     Recall: 0.6774\n","     F1:   0.7119\n","}\n","Modelo:  Clasificador MLP\n","Métricas: {\n","     Accuracy:  0.5581\n","     Precision: 0.7143\n","     Recall: 0.6452\n","     F1:   0.6780\n","}\n","Modelo:  Regresión logística\n","Métricas: {\n","     Accuracy:  0.5814\n","     Precision: 0.7241\n","     Recall: 0.6774\n","     F1:   0.7000\n","}\n","Modelo:  K vecinos\n","Métricas: {\n","     Accuracy:  0.6047\n","     Precision: 0.7333\n","     Recall: 0.7097\n","     F1:   0.7213\n","}\n","-----------------------\n","\n","Tipos de datos:  Datos clínicos y radiómicos\n","-----------------------\n","Modelo:  Red neuronal propia\n","Métricas: {\n","     Accuracy:  0.6977\n","     Precision: 0.7143\n","     Recall: 0.9677\n","     F1:   0.8219\n","}\n","Modelo:  Clasificador MLP\n","Métricas: {\n","     Accuracy:  0.6744\n","     Precision: 0.7073\n","     Recall: 0.9355\n","     F1:   0.8056\n","}\n","Modelo:  Regresión logística\n","Métricas: {\n","     Accuracy:  0.7209\n","     Precision: 0.7317\n","     Recall: 0.9677\n","     F1:   0.8333\n","}\n","Modelo:  K vecinos\n","Métricas: {\n","     Accuracy:  0.5116\n","     Precision: 0.8125\n","     Recall: 0.4194\n","     F1:   0.5532\n","}\n","-----------------------\n","\n","Tipos de datos:  Datos genómicos y radiómicos\n","-----------------------\n","Modelo:  Red neuronal propia\n","Métricas: {\n","     Accuracy:  0.7368\n","     Precision: 0.7368\n","     Recall: 1.0000\n","     F1:   0.8485\n","}\n","Modelo:  Clasificador MLP\n","Métricas: {\n","     Accuracy:  0.7105\n","     Precision: 0.7429\n","     Recall: 0.9286\n","     F1:   0.8254\n","}\n","Modelo:  Regresión logística\n","Métricas: {\n","     Accuracy:  0.7368\n","     Precision: 0.7368\n","     Recall: 1.0000\n","     F1:   0.8485\n","}\n","Modelo:  K vecinos\n","Métricas: {\n","     Accuracy:  0.5000\n","     Precision: 0.6957\n","     Recall: 0.5714\n","     F1:   0.6275\n","}\n","-----------------------\n","\n","Tipos de datos:  Datos clínicos, genómicos y radiómicos\n","-----------------------\n","Modelo:  Red neuronal propia\n","Métricas: {\n","     Accuracy:  0.7442\n","     Precision: 0.7381\n","     Recall: 1.0000\n","     F1:   0.8493\n","}\n","Modelo:  Clasificador MLP\n","Métricas: {\n","     Accuracy:  0.3023\n","     Precision: 1.0000\n","     Recall: 0.0323\n","     F1:   0.0625\n","}\n","Modelo:  Regresión logística\n","Métricas: {\n","     Accuracy:  0.7209\n","     Precision: 0.7317\n","     Recall: 0.9677\n","     F1:   0.8333\n","}\n","Modelo:  K vecinos\n","Métricas: {\n","     Accuracy:  0.5116\n","     Precision: 0.8125\n","     Recall: 0.4194\n","     F1:   0.5532\n","}\n","-----------------------\n","\n"]}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}